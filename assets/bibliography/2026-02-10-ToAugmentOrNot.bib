
@article{sedaghat2016orientation,
  title={Orientation-boosted voxel nets for 3D object recognition},
  author={Sedaghat, Nima and Zolfaghari, Mohammadreza and Amiri, Ehsan and Brox, Thomas},
  journal={arXiv preprint arXiv:1604.03351},
  year={2016}
}
@article{deng2021vector,
  title={Vector neurons: A general framework for so (3)-equivariant networks},
  author={Deng, Congyue and Litany, Or and Duan, Yueqi and Poulenard, Adrien and Tagliasacchi, Andrea and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12200--12209},
  year={2021}
}

@article{kaba2023equivariance,
  title={Equivariance with learned canonicalization functions},
  author={Kaba, S{\'e}kou-Oumar and Mondal, Arnab Kumar and Zhang, Yan and Bengio, Yoshua and Ravanbakhsh, Siamak},
  booktitle={International Conference on Machine Learning},
  pages={15546--15566},
  year={2023},
  organization={PMLR}
}

@ARTICLE{Kondor2018-nbody,
  title         = "N-body Networks: a Covariant Hierarchical Neural Network
                   Architecture for Learning Atomic Potentials",
  author        = "Kondor, Risi",
  abstract      = "We describe N-body networks, a neural network architecture
                   for learning the behavior and properties of complex many
                   body physical systems. Our specific application is to learn
                   atomic potential energy surfaces for use in molecular
                   dynamics simulations. Our architecture is novel in that (a)
                   it is based on a hierarchical decomposition of the many body
                   system into subsytems, (b) the activations of the network
                   correspond to the internal state of each subsystem, (c) the
                   ``neurons'' in the network are constructed explicitly so as
                   to guarantee that each of the activations is covariant to
                   rotations, (d) the neurons operate entirely in Fourier
                   space, and the nonlinearities are realized by tensor
                   products followed by Clebsch-Gordan decompositions. As part
                   of the description of our network, we give a
                   characterization of what way the weights of the network may
                   interact with the activations so as to ensure that the
                   covariance property is maintained.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1803.01588"
}

@article{jacot2018,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{wu20153d,
  title={3d shapenets: A deep representation for volumetric shapes},
  author={Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1912--1920},
  year={2015}
}

@misc{huang2025,
      title={Gaussian and Non-Gaussian Universality of Data Augmentation}, 
      author={Kevin Han Huang and Peter Orbanz and Morgane Austern},
      year={2025},
      eprint={2202.09134},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.09134}, 
}

@article{du2022se3,
  title = 	 {{SE}(3) Equivariant Graph Neural Networks with Complete Local Frames},
  author =       {Du, Weitao and Zhang, He and Du, Yuanqi and Meng, Qi and Chen, Wei and Zheng, Nanning and Shao, Bin and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5583--5608},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/du22e/du22e.pdf},
  url = 	 {https://proceedings.mlr.press/v162/du22e.html},
  abstract = 	 {Group equivariance (e.g. SE(3) equivariance) is a critical physical symmetry in science, from classical and quantum physics to computational biology. It enables robust and accurate prediction under arbitrary reference transformations. In light of this, great efforts have been put on encoding this symmetry into deep neural networks, which has been shown to improve the generalization performance and data efficiency for downstream tasks. Constructing an equivariant neural network generally brings high computational costs to ensure expressiveness. Therefore, how to better trade-off the expressiveness and computational efficiency plays a core role in the design of the equivariant deep learning models. In this paper, we propose a framework to construct SE(3) equivariant graph neural networks that can approximate the geometric quantities efficiently. Inspired by differential geometry and physics, we introduce equivariant local complete frames to graph neural networks, such that tensor information at given orders can be projected onto the frames. The local frame is constructed to form an orthonormal basis that avoids direction degeneration and ensure completeness. Since the frames are built only by cross product operations, our method is computationally efficient. We evaluate our method on two tasks: Newton mechanics modeling and equilibrium molecule conformation generation. Extensive experimental results demonstrate that our model achieves the best or competitive performance in two types of datasets.}
}


@article{dascoli2020,
  title = 	 {Double Trouble in Double Descent: Bias and Variance(s) in the Lazy Regime},
  author =       {D'Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2280--2290},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/d-ascoli20a/d-ascoli20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/d-ascoli20a.html},
  abstract = 	 {Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a "double descent"—a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond it which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. We demonstrate that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We quantify how they are suppressed by ensembling the outputs of $K$ independently initialized estimators. For $K\rightarrow \infty$, the test error is monotonously decreasing and remains constant beyond the interpolation threshold. We further compare the effects of overparametrizing, ensembling and regularizing. Finally, we present numerical experiments on classic deep learning setups to show that our results hold qualitatively in realistic lazy learning scenarios.}
}

@article{
atanasov2023,
title={The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes},
author={Alexander Atanasov and Blake Bordelon and Sabarish Sainathan and Cengiz Pehlevan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=JLINxPOVTh7}
}
@misc{atanasov2024scalingrenormalizationhighdimensionalregression,
      title={Scaling and renormalization in high-dimensional regression}, 
      author={Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan},
      year={2024},
      eprint={2405.00592},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.00592}, 
}

@misc{lyle2020,
      title={On the Benefits of Invariance in Neural Networks}, 
      author={Clare Lyle and Mark van der Wilk and Marta Kwiatkowska and Yarin Gal and Benjamin Bloem-Reddy},
      year={2020},
      eprint={2005.00178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.00178}, 
}

@article{chen2020,
  author  = {Shuxiao Chen and Edgar Dobriban and Jane H. Lee},
  title   = {A Group-Theoretic Framework for Data Augmentation},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {245},
  pages   = {1--71},
  url     = {http://jmlr.org/papers/v21/20-163.html}
}

@article{
bietti2021,
title={On the Sample Complexity of Learning under Geometric Stability},
author={Alberto Bietti and Luca Venturi and Joan Bruna},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=vlf0zTKa5Lh}
}

@article{mei2021,
  title = 	 {Learning with invariances in random features and kernel models},
  author =       {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {3351--3418},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/mei21a/mei21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/mei21a.html},
  abstract = 	 {A number of machine learning tasks entail a high degree of invariance: the data distribution does not change if we act on the data with a certain group of transformations. For instance, labels of images are invariant under translations of the images. Certain neural network architectures —for instance, convolutional networks—are believed to owe their success to the fact that they exploit such invariance properties. With the objective of quantifying the gain achieved by invariant architectures, we introduce two classes of models: invariant random features and invariant kernel methods. The latter includes, as a special case, the neural tangent kernel for convolutional networks with global average pooling. We consider uniform covariates distributions on the sphere and hypercube and a general invariant target function. We characterize the test error of invariant methods in a high-dimensional regime in which the sample size and number of hidden units scale as polynomials in the dimension, for a class of groups that we call ‘degeneracy $\alpha$’, with $\alpha \leq 1$. We show that exploiting invariance in the architecture saves a $d^\alpha$ factor ($d$ stands for the dimension) in sample size and number of hidden units to achieve the same test error as for unstructured architectures. Finally, we show that output symmetrization of an unstructured kernel estimator does not give a significant statistical improvement; on the other hand, data augmentation with an unstructured kernel estimator is equivalent to an invariant kernel estimator and enjoys the same improvement in statistical efficiency.}
}


@article{martinez2024,
title={Symmetries in Overparametrized Neural Networks: A Mean Field View},
author={Javier Maass Mart{\'\i}nez and Joaquin Fontbona},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=L86glqNCUj}
}

@article{chizat2019,
  author={Lénaïc Chizat and Edouard Oyallon and Francis R. Bach},
  title={On Lazy Training in Differentiable Programming},
  year={2019},
  cdate={1546300800000},
  pages={2933-2943},
  url={https://proceedings.neurips.cc/paper/2019/hash/ae614c557843b1df326cb29c57225459-Abstract.html},
  booktitle={NeurIPS},
  crossref={conf/nips/2019}
}

@article{hastie2022,
author = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
title = {{Surprises in high-dimensional ridgeless least squares interpolation}},
volume = {50},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {949 -- 986},
keywords = {interpolation, overparametrization, Random matrix theory, regression, Ridge regression},
year = {2022},
doi = {10.1214/21-AOS2133},
URL = {https://doi.org/10.1214/21-AOS2133}
}

@misc{atanasov2024riskcrossvalidationridge,
      title={Risk and cross validation in ridge regression with correlated samples}, 
      author={Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan},
      year={2024},
      eprint={2408.04607},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.04607}, 
}

@article{bach2024,
author = {Bach, Francis},
title = {High-Dimensional Analysis of Double Descent for Linear Regression with Random Projections},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {6},
number = {1},
pages = {26-50},
year = {2024},
doi = {10.1137/23M1558781},
URL = { 
        https://doi.org/10.1137/23M1558781
},
eprint = { 
        https://doi.org/10.1137/23M1558781
},
abstract = { Abstract. We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and review earlier results using classical notions from nonparametric statistics, namely, degrees of freedom, also known as effective dimensionality. We then compute asymptotic equivalents of the generalization performance (in terms of squared bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon. }
}

@ARTICLE{Owen2023-xw,
  title         = "Complexity of {Many-Body} Interactions in Transition Metals
                   via {Machine-Learned} Force Fields from the {TM23} Data Set",
  author        = "Owen, Cameron J and Torrisi, Steven B and Xie, Yu and
                   Batzner, Simon and Coulter, Jennifer and Musaelian, Albert
                   and Sun, Lixin and Kozinsky, Boris",
  abstract      = "This work examines challenges associated with the accuracy
                   of machine-learned force fields (MLFFs) for bulk solid and
                   liquid phases of d-block elements. In exhaustive detail, we
                   contrast the performance of force, energy, and stress
                   predictions across the transition metals for two leading
                   MLFF models: a kernel-based atomic cluster expansion method
                   implemented using Gaussian processes (FLARE), and an
                   equivariant message-passing neural network (NequIP). Early
                   transition metals present higher relative errors and are
                   more difficult to learn relative to late platinum- and
                   coinage-group elements, and this trend persists across model
                   architectures. The underlying physical and chemical
                   differences in the complexity of interatomic interactions
                   for different metals are revealed via comparison of the
                   performance of representations with different many-body
                   order and angular resolution. This work illustrates
                   challenges in capturing intricate properties of metallic
                   bonding with currently leading MLFFs and provides a
                   reference data set for transition metals, aimed at
                   benchmarking and development of emerging machine-learned
                   approximations.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cond-mat.mtrl-sci",
  eprint        = "2302.12993"
}

@article{elesedy2021provably,
  title={Provably strict generalisation benefit for equivariant models},
  author={Elesedy, Bryn and Zaidi, Sheheryar},
  booktitle={International conference on machine learning},
  pages={2959--2969},
  year={2021},
  organization={PMLR}
}

@ARTICLE{Frey2023-gg,
  title     = "Neural scaling of deep chemical models",
  author    = "Frey, Nathan C and Soklaski, Ryan and Axelrod, Simon and Samsi,
               Siddharth and G{\'o}mez-Bombarelli, Rafael and Coley, Connor W
               and Gadepally, Vijay",
  abstract  = "Massive scale, in terms of both data availability and
               computation, enables important breakthroughs in key application
               areas of deep learning such as natural language processing and
               computer vision. There is emerging evidence that scale may be a
               key ingredient in scientific deep learning, but the importance
               of physical priors in scientific domains makes the strategies
               and benefits of scaling uncertain. Here we investigate
               neural-scaling behaviour in large chemical models by varying
               model and dataset sizes over many orders of magnitude, studying
               models with over one billion parameters, pre-trained on datasets
               of up to ten million datapoints. We consider large language
               models for generative chemistry and graph neural networks for
               machine-learned interatomic potentials. We investigate the
               interplay between physical priors and scale and discover
               empirical neural-scaling relations for language models in
               chemistry with a scaling exponent of 0.17 for the largest
               dataset size considered, and a scaling exponent of 0.26 for
               equivariant graph neural network interatomic potentials. Deep
               learning methods in natural language processing generally become
               more effective with larger datasets and bigger networks. But it
               is not evident whether the same is true for more specialized
               domains such as cheminformatics. Frey and colleagues provide
               empirical explorations of chemistry models and find that
               neural-scaling laws hold true even for the largest tested models
               and datasets.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  5,
  number    =  11,
  pages     = "1297--1305",
  month     =  oct,
  year      =  2023,
  language  = "en"
}

% hypothesis tests for distributional group symmetry: https://arxiv.org/pdf/2307.15834.pdf
@article{chiu2023nonparametric,
author = {Chiu, Kenny and Bloem-Reddy, Benjamin},
title = {Non-Parametric Hypothesis Tests for Distributional Group Symmetry}, 
year = {2023},
booktitle = {NeurIPS AI for Science Workshop},
}

@ARTICLE{Batzner2022-sr,
  title     = "E(3)-equivariant graph neural networks for data-efficient and
               accurate interatomic potentials",
  author    = "Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger,
               Mario and Mailoa, Jonathan P and Kornbluth, Mordechai and
               Molinari, Nicola and Smidt, Tess E and Kozinsky, Boris",
  abstract  = "This work presents Neural Equivariant Interatomic Potentials
               (NequIP), an E(3)-equivariant neural network approach for
               learning interatomic potentials from ab-initio calculations for
               molecular dynamics simulations. While most contemporary
               symmetry-aware models use invariant convolutions and only act on
               scalars, NequIP employs E(3)-equivariant convolutions for
               interactions of geometric tensors, resulting in a more
               information-rich and faithful representation of atomic
               environments. The method achieves state-of-the-art accuracy on a
               challenging and diverse set of molecules and materials while
               exhibiting remarkable data efficiency. NequIP outperforms
               existing models with up to three orders of magnitude fewer
               training data, challenging the widely held belief that deep
               neural networks require massive training sets. The high data
               efficiency of the method allows for the construction of accurate
               potentials using high-order quantum chemical level of theory as
               reference and enables high-fidelity molecular dynamics
               simulations over long time scales.",
  journal   = "Nat. Commun.",
  publisher = "Nature Publishing Group",
  volume    =  13,
  number    =  1,
  pages     = "2453",
  month     =  may,
  year      =  2022,
  language  = "en"
}



@ARTICLE{Rackers2023-sb,
  title     = "A recipe for cracking the quantum scaling limit with machine
               learned electron densities",
  author    = "Rackers, Joshua A and Tecot, Lucas and Geiger, Mario and Smidt,
               Tess E",
  abstract  = "A long-standing goal of science is to accurately simulate large
               molecular systems using quantum mechanics. The poor scaling of
               current quantum chemistry algorithms on classical computers,
               however, imposes an effective limit of about a few dozen atoms
               on traditional electronic structure calculations. We present a
               machine learning (ML) method to break through this scaling limit
               for electron densities. We show that Euclidean neural networks
               can be trained to predict molecular electron densities from
               limited data. By learning the electron density, the model can be
               trained on small systems and make accurate predictions on large
               ones. In the context of water clusters, we show that an ML model
               trained on clusters of just 12 molecules contains all the
               information needed to make accurate electron density predictions
               on cluster sizes of 50 or more, beyond the scaling limit of
               current quantum chemistry methods.",
  journal   = "Mach. Learn.: Sci. Technol.",
  publisher = "IOP Publishing",
  volume    =  4,
  number    =  1,
  pages     = "015027",
  month     =  feb,
  year      =  2023,
  language  = "en"
}


@article{yarotsky2021universal,
  title={Universal approximations of invariant maps by neural networks},
  author={Yarotsky, Dmitry},
  journal={Constructive Approximation},
  pages={1--68},
  year={2021},
  publisher={Springer}
}

@article{qm9datasetpaper,
  title={Quantum chemistry structures and properties of 134 kilo molecules},
  author={Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and von Lilienfeld, O Anatole},
  journal={Scientific Data},
  volume={1},
  year={2014},
  publisher={Nature Publishing Group}
}

@article{mcneela2023almost,
  title={Almost Equivariance via Lie Algebra Convolutions},
  author={McNeela, Daniel},
  journal={arXiv preprint arXiv:2310.13164},
  year={2023}
}

@article{d2021convit,
  title={Convit: Improving vision transformers with soft convolutional inductive biases},
  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  journal={ICML},
  year={2021}
}

@article{holderrieth2021equivariant,
  title={Equivariant learning of stochastic fields: Gaussian processes and steerable conditional neural processes},
  author={Holderrieth, Peter and Hutchinson, Michael J and Teh, Yee Whye},
  booktitle={International Conference on Machine Learning},
  pages={4297--4307},
  year={2021},
  organization={PMLR}
}

@article{Cohen2016Group,
title={Group Equivariant Convolutional Networks},
author={T.S. Cohen and M. Welling},
booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
year={2016}
}


@article{weiler20183d,
  title={3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data},
  author={Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  booktitle={NeurIPS},
  year={2018}
}

@article{Garcia2021EN,
  title={E(n) Equivariant Graph Neural Networks},
  author={Victor Garcia Satorras and Emiel Hoogeboom and Max Welling},
  journal={arXiv preprint arXiv:2102.09844},
  year={2021}
}

@article{
Elsayed2020Revisiting,
title={Revisiting Spatial Invariance with Low-Rank Local Connectivity},
author={Gamaleldin F. Elsayed and Prajit Ramachandran and Jonathon Shlens and Simon Kornblith},
booktitle={Proceedings of the 37th International Conference of Machine Learning (ICML)},
year={2020}
}

@article{guo2012lagrange,
  title={Lagrange structure and dynamics for solutions to the spherically symmetric compressible Navier-Stokes equations},
  author={Guo, Zhenhua and Li, Hai-Liang and Xin, Zhouping},
  journal={Communications in Mathematical Physics},
  volume={309},
  pages={371--412},
  year={2012},
  publisher={Springer}
}

@article{woodward1997octahedral,
  title={Octahedral tilting in perovskites. I. Geometrical considerations},
  author={Woodward, Patrick M},
  journal={Acta Crystallographica Section B: Structural Science},
  volume={53},
  number={1},
  pages={32--43},
  year={1997},
  publisher={International Union of Crystallography}
}

@article{van19922p,
  title={The 2p absorption spectra of 3d transition metal compounds in tetrahedral and octahedral symmetry},
  author={Van der Laan, G and Kirkman, IW},
  journal={Journal of Physics: Condensed Matter},
  volume={4},
  number={16},
  pages={4189},
  year={1992},
  publisher={IOP Publishing}
}

@article{nabarro1947dislocations,
  title={Dislocations in a simple cubic lattice},
  author={Nabarro, FRN3374614},
  journal={Proceedings of the Physical Society},
  volume={59},
  number={2},
  pages={256},
  year={1947},
  publisher={IOP Publishing}
}

@article{petrache2023approximation,
  title={Approximation-Generalization Trade-offs under (Approximate) Group Equivariance},
  author={Petrache, Mircea and Trivedi, Shubhendu},
  journal={arXiv preprint arXiv:2305.17592},
  year={2023}
}

@article{sadowski1994comparison,
  author    = {Sadowski, J. and Gasteiger, J. and Klebe, G.},
  title     = {Comparison of Automatic Three-Dimensional Model Builders Using 639 X-Ray Structures},
  journal   = {Journal of Chemical Information and Computer Sciences},
  year      = {1994},
  volume    = {34},
  pages     = {1000--1008},
  doi       = {10.1021/ci00020a039}
}

@article{schwab2010conformations,
  author    = {Schwab, C. H.},
  title     = {Conformations and 3D pharmacophore searching},
  journal   = {Drug Discovery Today: Technologies},
  year      = {2010},
  volume    = {7},
  number    = {4},
  pages     = {e245--e253},
  doi       = {10.1016/j.ddtec.2010.10.003},
  note      = {Winter 2010}
}

@misc{corina,
  title     = {{3D Structure Generator CORINA Classic}},
  author    = {{Molecular Networks Altamira}},
  howpublished = {\url{http://www.mn-am.com}},
  note      = {MN-AM, Nuremberg, Germany}
}

@article{graphormershi2022benchmarking,
  title={Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets},
  author={Yu Shi and Shuxin Zheng and Guolin Ke and Yifei Shen and Jiacheng You and Jiyan He and Shengjie Luo and Chang Liu and Di He and Tie-Yan Liu},
  journal={arXiv preprint arXiv:2203.04810},
  year={2022},
  url={https://arxiv.org/abs/2203.04810}
}

@article{
graphormerying2021do,
title={Do Transformers Really Perform Badly for Graph Representation?},
author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=OeWooOxFwDa}
}

@software{kleinhenz2025e3tools,
  author       = {Joseph Kleinhenz and Ameya Daigavane},
  title        = {e3tools},
  version      = {0.1.1},
  date         = {2025-04-04},
  url          = {https://github.com/prescient-design/e3tools/tree/main}
}

@article{qm7byang2019quantum,
  title={Quantum mechanical static dipole polarizabilities in the QM7b and AlphaML showcase databases},
  author={Yang, Yuezhi and Lao, Ka Un and Wilkins, David M and DiStasio Jr, Robert A and Tkatchenko, Alexandre},
  journal={Scientific Data},
  volume={6},
  number={1},
  pages={152},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41597-019-0157-8}
}

@article{qm7original1,
  author  = {L. C. Blum and J.-L. Reymond},
  title   = {{970 Million Druglike Small Molecules for Virtual Screening in the Chemical Universe Database GDB-13}},
  journal = {Journal of the American Chemical Society},
  volume  = {131},
  pages   = {8732--8733},
  year    = {2009},
  doi     = {10.1021/ja902302h}
}


@article{qm7original2,
  author={Gr{\'e}goire Montavon and Matthias Rupp and Vivekanand Gobre and Alvaro Vazquez-Mayagoitia and Katja Hansen and Alexandre
Tkatchenko and Klaus-Robert M{\"u}ller and O Anatole von Lilienfeld},
  title={Machine learning of molecular electronic properties in chemical compound space},
  journal={New Journal of Physics},
  volume={15},
  number={9},
  pages={095003},
  url={http://stacks.iop.org/1367-2630/15/i=9/a=095003},
  year={2013}
}

@article{
Cohen2020STEERABLE,
title={STEERABLE CNNS},
author={Taco S. Cohen and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017}
}

@article{wang2020incorporating,
  title={Incorporating symmetry into deep dynamics models for improved generalization},
  author={Rui Wang and Robin Walters and Rose Yu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}



@article{helwig2023group,
  title={Group Equivariant Fourier Neural Operators for Partial Differential Equations},
  author={Helwig, Jacob and Zhang, Xuan and Fu, Cong and Kurtin, Jerry and Wojtowytsch, Stephan and Ji, Shuiwang},
  journal={arXiv preprint arXiv:2306.05697},
  year={2023}
}

@article{liao2022equiformer,
  title={Equiformer: Equivariant graph attention transformer for 3d atomistic graphs},
  author={Liao, Yi-Lun and Smidt, Tess},
  journal={arXiv preprint arXiv:2206.11990},
  year={2022}
}

@article{liao2023equiformerv2,
  title={EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations},
  author={Liao, Yi-Lun and Wood, Brandon and Das, Abhishek and Smidt, Tess},
  journal={arXiv preprint arXiv:2306.12059},
  year={2023}
}

@book{kosmann2011noether,
  title={The Noether Theorems},
  author={Kosmann-Schwarzbach, Yvette and Schwarzbach, Bertram E and Kosmann-Schwarzbach, Yvette},
  year={2011},
  publisher={Springer}
}

@article{geiger2022e3nn,
  title={e3nn: Euclidean neural networks},
  author={Geiger, Mario and Smidt, Tess},
  journal={arXiv preprint arXiv:2207.09453},
  year={2022}
}
@article{gasteiger2022gemnet,
  title={Gemnet-oc: developing graph neural networks for large and diverse molecular simulation datasets},
  author={Gasteiger, Johannes and Shuaibi, Muhammed and Sriram, Anuroop and G{\"u}nnemann, Stephan and Ulissi, Zachary and Zitnick, C Lawrence and Das, Abhishek},
  journal={arXiv preprint arXiv:2204.02782},
  year={2022}
}

@article{bao2019equivariant,
  title={Equivariant neural networks and equivarification},
  author={Erkao Bao and Linqi Song},
  journal={arXiv preprint arXiv:1906.07172},
  year={2019}
}

@article{huang2023approximately,
  title={Approximately Equivariant Graph Networks},
  author={Huang, Ningyuan and Levie, Ron and Villar, Soledad},
  journal={arXiv preprint arXiv:2308.10436},
  year={2023}
}


@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{passaro2023reducing,
  title={Reducing SO (3) Convolutions to SO (2) for Efficient Equivariant GNNs},
  author={Passaro, Saro and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:2302.03655},
  year={2023}
}
@article{schutt2017schnet,
  title={Schnet: A continuous-filter convolutional neural network for modeling quantum interactions},
  author={Sch{\"u}tt, Kristof and Kindermans, Pieter-Jan and Sauceda Felix, Huziel Enoc and Chmiela, Stefan and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zitnick2022spherical,
  title={Spherical channels for modeling atomic interactions},
  author={Zitnick, Larry and Das, Abhishek and Kolluru, Adeesh and Lan, Janice and Shuaibi, Muhammed and Sriram, Anuroop and Ulissi, Zachary and Wood, Brandon},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8054--8067},
  year={2022}
}
@article{
liu2022spherical,
title={Spherical Message Passing for 3D Molecular Graphs},
author={Yi Liu and Limei Wang and Meng Liu and Yuchao Lin and Xuan Zhang and Bora Oztekin and Shuiwang Ji},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=givsRXsOt9r}
}

@article{thomas2018tensor,
  title={Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author={Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  journal={arXiv preprint arXiv:1802.08219},
  year={2018}
}

@article{Walters2021ECCO,
  title={Trajectory Prediction using Equivariant Continuous Convolution},
  author={Robin Walters and Jinxi Li and Rose Yu},
  journal={International Conference on Learning Representations},
  year={2021},
}

@article{
finzi2021residual,
title={Residual Pathway Priors for Soft Equivariance Constraints},
author={Marc Anton Finzi and Gregory Benton and Andrew Gordon Wilson},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=k505ekjMzww}
}

@article{finzi2021emlp,
  title={A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups},
  author={Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
  journal={International Conference on Machine Learning},
  year={2021}
}

@article{Ghosh19Scale,
  title={Scale Steerable Filters for Locally Scale-Invariant Convolutional Neural Networks},
  author={Rohan Ghosh and Anupam K. Gupta},
  journal={arXiv preprint arXiv:1906.03861},
  year={2019}
}


@article{
    Sosnovik2020Scale-Equivariant,
    title={Scale-Equivariant Steerable Networks},
    author={Ivan Sosnovik and Michał Szmaja and Arnold Smeulders},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=HJgpugrKPS}
}

@article{knigge2022exploiting,
  title={Exploiting redundancy: Separable group convolutional networks on lie groups},
  author={Knigge, David M and Romero, David W and Bekkers, Erik J},
  booktitle={International Conference on Machine Learning},
  pages={11359--11386},
  year={2022},
  organization={PMLR}
}

@article{ratner2017learning,
  title={Learning to compose domain-specific transformations for data augmentation},
  author={Ratner, Alexander J and Ehrenberg, Henry R and Hussain, Zeshan and Dunnmon, Jared and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={3239},
  year={2017},
  publisher={NIH Public Access}
}


@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={245},
  pages={1--71},
  year={2020}
}

@article{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
  booktitle={International Conference on Machine Learning},
  pages={2892--2901},
  year={2017},
  organization={PMLR}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv:2104.13478},
  year={2021}
}


@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{bruna2013spectral,
  title={Spectral networks and locally connected networks on graphs},
  author={Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6203},
  year={2013}
}

@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}

@article{maron2018invariant,
  title={Invariant and equivariant graph networks},
  author={Maron, Haggai and Ben-Hamu, Heli and Shamir, Nadav and Lipman, Yaron},
  journal={arXiv preprint arXiv:1812.09902},
  year={2018}
}


@article{esteves2018learning,
  title={Learning so (3) equivariant representations with spherical cnns},
  author={Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={52--68},
  year={2018}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  journal={arXiv preprint arXiv:1703.06114},
  year={2017}
}

@article{worrall2017harmonic,
  title={Harmonic networks: Deep translation and rotation equivariance},
  author={Worrall, Daniel E and Garbin, Stephan J and Turmukhambetov, Daniyar and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5028--5037},
  year={2017}
}


@article{satorras2021n,
  title={E (n) equivariant graph neural networks},
  author={Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}


@article{Anderson2019Cormorant,
  title={Cormorant: Covariant Molecular Neural Networks},
  author={Brandon Anderson and Truong-Son Hy and Risi Kondor},
  booktitle={Advances in neural information processing systems (NeurIPS)},
  year={2019}
}

@article{cohen2018general,
  title={A general theory of equivariant cnns on homogeneous spaces},
  author={Cohen, Taco and Geiger, Mario and Weiler, Maurice},
  journal={arXiv preprint arXiv:1811.02017},
  year={2018}
}

@article{maron2020learning,
  title={On learning sets of symmetric elements},
  author={Maron, Haggai and Litany, Or and Chechik, Gal and Fetaya, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={6734--6744},
  year={2020},
  organization={PMLR}
}


@article{
Sosnovik2020Scale,
title={Scale-Equivariant Steerable Networks},
author={Ivan Sosnovik and Michał Szmaja and Arnold Smeulders},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJgpugrKPS}
}



@article{Shi2017DeepLF,
  title={Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model},
  author={Xingjian Shi and Zhihan Gao and Leonard Lausen and Hao Wang and D. Yeung and W. Wong and Wang-chun Woo},
  booktitle={Advances in neural information processing systems},
  year={2017}
}



@article{eulerian,
   author={Tompson, Jonathan and Schlachter, Kristofer and Sprechmann, Pablo and Perlin, Ken},
   year={2017},
   title={Accelerating {E}ulerian fluid simulation with convolutional networks},
   booktitle={ICML'17 Proceedings of the 34th International Conference on Machine Learning },
   pages={3424-3433},
   volume = {70}
}



@article{manek2020learning,
 author = {Kolter, J. Zico and Manek, Gaurav},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 pages = {11128--11136},
 title = {Learning Stable Deep Dynamics Models},
 volume = {32},
 year = {2019}
}


@article{li2020fourier,
  title={Fourier Neural Operator for Parametric Partial Differential Equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={International Conference on Learning Representations},
  year={2021}
}

@article{lutter2018deep,
  title={Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning},
  author={Lutter, Michael and Ritter, Christian and Peters, Jan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{
pfaff2021learning,
title={Learning Mesh-Based Simulation with Graph Networks},
author={Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=roNqYL0_XP}
}

@article{raissi2017physics,
  title={Physics Informed Deep Learning (Part I): Data-driven solutions of nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1711.10561},
  year={2017}
}

@article{phiflow,
  title={phiflow: A Differentiable PDE Solving Framework for
Deep Learning via Physical Simulations},
  author={Philipp M Holl and Kiwon Um and Nils Thuerey},
  booktitle={Workshop on Differentiable Vision, Graphics, and Physics in Machine Learning at NeurIPS},
  year={2020}
}

@article{moyer2018adversarial,
  title={Invariant representations without adversarial training},
  author={Daniel Moyer and Shuyang Gao and Rob Brekelmans and Aram Galstyan and Greg Ver Steeg},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={9084--9093},
  year={2018}
}

@article{worrall2019deep,
  title={Deep scale-spaces: Equivariance over scale},
  author={Daniel Worrall and Max Welling},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={7364--7376},
  year={2019}
}

@article{cohen2019gauge,
  title = 	 {Gauge Equivariant Convolutional Networks and the Icosahedral {CNN}},
  author = 	 {Taco S. Cohen and Maurice Weiler and Berkay Kicanaoglu and Max Welling},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  pages = 	 {1321--1330},
  year = 	 {2019},
  volume = 	 {97}
}

@article{cohen2016steerable,
  title={Steerable {CNN}s},
  author={Taco S. Cohen and Max Welling},
  journal={arXiv preprint arXiv:1612.08498},
  year={2016}
}

@article{weiler2019e2cnn,
  title={General {E}(2)-Equivariant Steerable {CNNs}},
  author={Maurice Weiler and Gabriele Cesa},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={14334--14345},
  year={2019}
}



@article{weiler2018learning,
  title={Learning steerable filters for rotation equivariant {CNN}s},
  author={Maurice Weiler and Fred A. Hamprecht and Martin Storath},
  journal={Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@article{chidester2018rotation,
  title={Rotation Equivariance and Invariance in Convolutional Neural Networks},
  author={Benjamin Chidester and Minh N. Do and Jian Ma},
  journal={arXiv preprint arXiv:1805.12301},
  year={2018}
}

@article{Lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Karel Lenc and Andrea Vedaldi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}


@article{kondor2018generalization,
  title = 	 {On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups},
  author = 	 {Risi Kondor and Shubhendu Trivedi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  pages = 	 {2747--2755},
  year = 	 {2018},
  volume = 	 {80}
}

@article{cohen2019general,
  title={A general theory of equivariant cnns on homogeneous spaces},
  author={Taco S Cohen and Mario Geiger and Maurice Weiler},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9142--9153},
  year={2019}
}

@article{cohen2018generalization,
  title={Quantifying Translation-Invariance in Convolutional Neural Networks},
  author={Eric Kauderer-Abrams},
  journal={arXiv preprint arXiv:1801.01450},
  year={2018}
}

@article{sabour2017dynamic,
  title={Dynamic routing between capsules},
  author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1710.09829},
  year={2017}
}



@article{ling2016ra,
  title={Reynolds Averaged Turbulence Modeling using Deep Neural Networks with Embedded Invariance},
  author={Julia Ling and Andrew Kurzawskim and Jeremy Templeton},
  journal={Journal of Fluid Mechanics},
  year={2017}
}

@article{zhang2019euclidean,
   author={Xinhua Zhang and Lance Williams},
   year={2019},
   title={Euclidean Invariant Recognition of {2D} Shapes Using Histograms of Magnitudes of Local {F}ourier-{M}ellin Descriptors},
   booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
   pages={303-311},
   volume = {1}
}

@article{dieleman2016cyclic,
  title={Exploiting cyclic symmetry in convolutional neural networks},
  author={Sander Dieleman and Jeffrey De Fauw and Koray Kavukcuoglu},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2016}
}

@article{dehmamy2021automatic,
  title={Automatic symmetry discovery with lie algebra convolutional network},
  author={Dehmamy, Nima and Walters, Robin and Liu, Yanchen and Wang, Dashun and Yu, Rose},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2503--2515},
  year={2021}
}

@article{zhang1988shift,
  title={Shift-invariant pattern recognition neural network and its optical architecture},
  author={Zhang, Wei},
  booktitle={Proceedings of annual conference of the Japan Society of Applied Physics},
  year={1988}
}

@article{Greydanus2019HamiltonianNN,
  title={Hamiltonian Neural Networks},
  author={S. Greydanus and Misko Dzamba and J. Yosinski},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.01563}
}


@article{Cranmer2020LagrangianNN,
  title={Lagrangian Neural Networks},
  author={M. Cranmer and S. Greydanus and S. Hoyer and P. Battaglia and D. Spergel and S. Ho},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.04630}
}


@article{zhang1990parallel,
  title={Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
  author={Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
  journal={Applied optics},
  volume={29},
  number={32},
  pages={4790--4797},
  year={1990},
  publisher={Optical Society of America}
}

@article{kanov2015johns,
  title={The Johns Hopkins turbulence databases: An open simulation laboratory for turbulence research},
  author={Kanov, Kalin and Burns, Randal and Lalescu, Cristian and Eyink, Gregory},
  journal={Computing in Science \& Engineering},
  volume={17},
  number={5},
  pages={10--17},
  year={2015},
  publisher={IEEE}
}

@article{finzi2020generalizing,
  title={Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
  author={Marc Finzi and Samuel Stanton and Pavel Izmailov and Andrew Gordon Wilson},
  journal={arXiv preprint arXiv:2002.12880},
  year={2020}
}


@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}


@article{scarselli2008graph,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}

@article{Wang2020TF,
   title={Towards Physics-informed Deep Learning for Turbulent Flow Prediction},
   author={Rui Wang and Karthik Kashinath and Mustafa Mustafa and Adrian Albert and Rose Yu},
   journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   Year = {2020}
}	

@article{benidis2022deep,
  title={Deep learning for time series forecasting: Tutorial and literature survey},
  author={Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and Bohlke-Schneider, Michael and Salinas, David and Stella, Lorenzo and others},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}

@article{jain2013commentary,
  title={Commentary: The Materials Project: A materials genome approach to accelerating materials innovation},
  author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and others},
  journal={APL materials},
  volume={1},
  number={1},
  year={2013},
  publisher={AIP Publishing}
}

@book{onuki2002phase,
  title={Phase transition dynamics},
  author={Onuki, Akira},
  year={2002},
  publisher={Cambridge University Press}
}

@article{wang2022approximately,
  title={Approximately equivariant networks for imperfectly symmetric dynamics},
  author={Wang, Rui and Walters, Robin and Yu, Rose},
  booktitle={International Conference on Machine Learning},
  pages={23078--23091},
  year={2022},
  organization={PMLR}
}

@article{dym2024equivariant,
author = {Dym, Nadav and Lawrence, Hannah and Siegel, Jonathan W},
title = {Equivariant frames and the impossibility of continuous canonicalization},
year = {2024},
publisher = {JMLR.org},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {486},
numpages = {40},
location = {Vienna, Austria},
series = {ICML'24}
}

@misc{xu2020theory,
      title={A Theory of Usable Information Under Computational Constraints}, 
      author={Yilun Xu and Shengjia Zhao and Jiaming Song and Russell Stewart and Stefano Ermon},
      year={2020},
    journal={ICLR},
      eprint={2002.10689},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.10689}, 
}

@article{xu2017information,
 author = {Xu, Aolin and Raginsky, Maxim},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Information-theoretic analysis of generalization capability of learning algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{tishby99information,
  added-at = {2017-09-27T13:23:06.000+0200},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  biburl = {https://www.bibsonomy.org/bibtex/2c61af806ab3a8fe92154753e84736818/mo_xime},
  booktitle = {Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing},
  comment = {cte: information bottleneck},
  interhash = {15bd5efbf394791da00b09839b9a5757},
  intrahash = {c61af806ab3a8fe92154753e84736818},
  keywords = {information},
  pages = {368-377},
  timestamp = {2017-09-27T15:48:05.000+0200},
  title = {The information bottleneck method},
  url = {https://arxiv.org/abs/physics/0004057},
  year = 1999
}


@article{smidt2021finding,
  title={Finding symmetry breaking order parameters with Euclidean neural networks},
  author={Smidt, Tess E and Geiger, Mario and Miller, Benjamin Kurt},
  journal={Physical Review Research},
  volume={3},
  number={1},
  pages={L012002},
  year={2021},
  publisher={APS}
}

@article{wang2023general,
  title={A general theory of correct, incorrect, and extrinsic equivariance},
  author={Wang, Dian and Zhu, Xupeng and Park, Jung Yeon and Jia, Mingxi and Su, Guanang and Platt, Robert and Walters, Robin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{nguyen2001computationally,
  title={A computationally efficient superresolution image reconstruction algorithm},
  author={Nguyen, Nhat and Milanfar, Peyman and Golub, Gene},
  journal={IEEE transactions on image processing},
  volume={10},
  number={4},
  pages={573--583},
  year={2001},
  publisher={IEEE}
}

@article{tian2011survey,
  title={A survey on super-resolution imaging},
  author={Tian, Jing and Ma, Kai-Kuang},
  journal={Signal, Image and Video Processing},
  volume={5},
  pages={329--342},
  year={2011},
  publisher={Springer}
}

@article{park2003super,
  title={Super-resolution image reconstruction: a technical overview},
  author={Park, Sung Cheol and Park, Min Kyu and Kang, Moon Gi},
  journal={IEEE signal processing magazine},
  volume={20},
  number={3},
  pages={21--36},
  year={2003},
  publisher={IEEE}
}

@article{fukami2023super,
  title={Super-resolution analysis via machine learning: a survey for fluid flows},
  author={Fukami, Kai and Fukagata, Koji and Taira, Kunihiko},
  journal={Theoretical and Computational Fluid Dynamics},
  pages={1--24},
  year={2023},
  publisher={Springer}
}

@article{werhahn2019multi,
  title={A multi-pass GAN for fluid flow super-resolution},
  author={Werhahn, Maximilian and Xie, You and Chu, Mengyu and Thuerey, Nils},
  journal={Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  volume={2},
  number={2},
  pages={1--21},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{xie2018tempogan,
  title={tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow},
  author={Xie, You and Franz, Erik and Chu, Mengyu and Thuerey, Nils},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--15},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{fukami2019super,
  title={Super-resolution reconstruction of turbulent flows with machine learning},
  author={Fukami, Kai and Fukagata, Koji and Taira, Kunihiko},
  journal={Journal of Fluid Mechanics},
  volume={870},
  pages={106--120},
  year={2019},
  publisher={Cambridge University Press}
}

@article{gao2021super,
  title={Super-resolution and denoising of fluid flow using physics-informed convolutional neural networks without high-resolution labels},
  author={Gao, Han and Sun, Luning and Wang, Jian-Xun},
  journal={Physics of Fluids},
  volume={33},
  number={7},
  year={2021},
  publisher={AIP Publishing}
}

@article{shu2023physics,
  title={A physics-informed diffusion model for high-fidelity flow field reconstruction},
  author={Shu, Dule and Li, Zijie and Farimani, Amir Barati},
  journal={Journal of Computational Physics},
  volume={478},
  pages={111972},
  year={2023},
  publisher={Elsevier}
}

@article{qian2021pu,
  title={Pu-gcn: Point cloud upsampling using graph convolutional networks},
  author={Qian, Guocheng and Abualshour, Abdulellah and Li, Guohao and Thabet, Ali and Ghanem, Bernard},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11683--11692},
  year={2021}
}

@article{erichson2020shallow,
  title={Shallow neural networks for fluid flow reconstruction with limited sensors},
  author={Erichson, N Benjamin and Mathelin, Lionel and Yao, Zhewei and Brunton, Steven L and Mahoney, Michael W and Kutz, J Nathan},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2238},
  pages={20200097},
  year={2020},
  publisher={The Royal Society Publishing}
}

@article{yasuda2022rotationally,
  title={Rotationally Equivariant Super-Resolution of Velocity Fields in Two-Dimensional Fluids Using Convolutional Neural Networks},
  author={Yasuda, Yuki and Onishi, Ryo},
  journal={arXiv e-prints},
  pages={arXiv--2202},
  year={2022}
}

@book{bradley2010mathematical,
  title={The mathematical theory of symmetry in solids: representation theory for point groups and space groups},
  author={Bradley, Christopher and Cracknell, Arthur},
  year={2010},
  publisher={Oxford University Press}
}


@article{desai2022symmetry,
  title={Symmetry discovery with deep learning},
  author={Desai, Krish and Nachman, Benjamin and Thaler, Jesse},
  journal={Physical Review D},
  volume={105},
  number={9},
  pages={096031},
  year={2022},
  publisher={APS}
}

@book{strocchi2005symmetry,
  title={Symmetry breaking},
  author={Strocchi, Franco},
  volume={643},
  year={2005},
  publisher={Springer}
}
@article{albrecht1982cosmology,
  title={Cosmology for grand unified theories with radiatively induced symmetry breaking},
  author={Albrecht, Andreas and Steinhardt, Paul J},
  journal={Physical Review Letters},
  volume={48},
  number={17},
  pages={1220},
  year={1982},
  publisher={APS}
}
@article{crawford1991symmetry,
  title={Symmetry and symmetry-breaking bifurcations in fluid dynamics},
  author={Crawford, John David and Knobloch, Edgar},
  journal={Annual Review of Fluid Mechanics},
  volume={23},
  number={1},
  pages={341--387},
  year={1991},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{yang2023generative,
  title={Generative Adversarial Symmetry Discovery},
  author={Yang, Jianke and Walters, Robin and Dehmamy, Nima and Yu, Rose},
  journal={arXiv preprint arXiv:2302.00236},
  year={2023}
}

@article{ying2018stochastic,
  title={Stochastic learning under random reshuffling with constant step-sizes},
  author={Ying, Bicheng and Yuan, Kun and Vlaski, Stefan and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={2},
  pages={474--489},
  year={2018},
  publisher={IEEE}
}

@article{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{
zhou2021metalearning,
title={Meta-learning Symmetries by Reparameterization},
author={Allan Zhou and Tom Knowles and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=-QxT4mJdijq}
}

@article{otto2023unified,
  title={A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning},
  author={Otto, Samuel E and Zolman, Nicholas and Kutz, J Nathan and Brunton, Steven L},
  journal={arXiv preprint arXiv:2311.00212},
  year={2023}
}

@article{yang2023latent,
  title={Latent Space Symmetry Discovery},
  author={Yang, Jianke and Dehmamy, Nima and Walters, Robin and Yu, Rose},
  journal={arXiv preprint arXiv:2310.00105},
  year={2023}
}

@book{batchelor1953theory,
  title={The theory of homogeneous turbulence},
  author={Batchelor, George Keith},
  year={1953},
  publisher={Cambridge university press}
}

@book{zakharov2012kolmogorov,
  title={Kolmogorov spectra of turbulence I: Wave turbulence},
  author={Zakharov, Vladimir E and L'vov, Victor S and Falkovich, Gregory},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{jucha2014time,
  title={Time-reversal-symmetry breaking in turbulence},
  author={Jucha, Jennifer and Xu, Haitao and Pumir, Alain and Bodenschatz, Eberhard},
  journal={Physical review letters},
  volume={113},
  number={5},
  pages={054501},
  year={2014},
  publisher={APS}
}

@article{pope2001turbulent,
  title={Turbulent flows},
  author={Pope, Stephen B},
  journal={Measurement Science and Technology},
  volume={12},
  number={11},
  pages={2020--2021},
  year={2001}
}


@article{lamb1998time,
  title={Time-reversal symmetry in dynamical systems: a survey},
  author={Lamb, Jeroen SW and Roberts, John AG},
  journal={Physica D: Nonlinear Phenomena},
  volume={112},
  number={1-2},
  pages={1--39},
  year={1998},
  publisher={Elsevier}
}

@article{soleymani,
  title = 	 {A Robust Kernel Statistical Test of Invariance: Detecting Subtle Asymmetries},
  author =       {Soleymani, Ashkan and Tahmasebi, Behrooz and Jegelka, Stefanie and Jaillet, Patrick},
  booktitle = 	 {Proceedings of The 28th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4816--4824},
  year = 	 {2025},
  editor = 	 {Li, Yingzhen and Mandt, Stephan and Agrawal, Shipra and Khan, Emtiyaz},
  volume = 	 {258},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--05 May},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v258/main/assets/soleymani25a/soleymani25a.pdf},
  url = 	 {https://proceedings.mlr.press/v258/soleymani25a.html},
  abstract = 	 {While invariances naturally arise in almost any type of real-world data, no efficient and robust test exists for detecting them in observational data under arbitrarily given group actions. We tackle this problem by studying measures of invariance that can capture even negligible underlying patterns. Our first contribution is to show that, while detecting subtle asymmetries is computationally intractable, a randomized method can be used to robustly estimate closeness measures to invariance within constant factors. This provides a general framework for robust statistical tests of invariance. Despite the extensive and well-established literature, our methodology, to the best of our knowledge, is the first to provide statistical tests for general group invariances with finite-sample guarantees on Type II errors. In addition, we focus on kernel methods and propose deterministic algorithms for robust testing with respect to both finite and infinite groups, accompanied by a rigorous analysis of their convergence rates and sample complexity. Finally, we revisit the general framework in the specific case of kernel methods, showing that recent closeness measures to invariance, defined via group averaging, are provably robust, leading to powerful randomized algorithms.}
}


@article{wilk2018,
 author = {van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Invariances using the Marginal Likelihood},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{immer2022,
 author = {Immer, Alexander and van der Ouderaa, Tycho and R\"{a}tsch, Gunnar and Fortuin, Vincent and van der Wilk, Mark},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {12449--12463},
 publisher = {Curran Associates, Inc.},
 title = {Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/50d005f92a6c5c9646db4b761da676ba-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{luke1998time,
  title={Time-reversal symmetry-breaking superconductivity in Sr2RuO4},
  author={Luke, G Ml and Fudamoto, Y and Kojima, KM and Larkin, MI and Merrin, J and Nachumi, B and Uemura, YJ and Maeno, Y and Mao, ZQ and Mori, Y and others},
  journal={Nature},
  volume={394},
  number={6693},
  pages={558--561},
  year={1998},
  publisher={Nature Publishing Group UK London}
}

@article{musaelian2023local,
  author    = {Musaelian, A. and Batzner, S. and Johansson, A. and Sun, L. and Owen, C. J. and Kornbluth, M. and Kozinsky, B.},
  title     = {Learning local equivariant representations for large-scale atomistic dynamics},
  journal   = {Nature Communications},
  year      = {2023},
  volume    = {14},
  number    = {1},
  pages     = {579},
  month     = feb,
  doi       = {10.1038/s41467-023-36329-y},
  issn      = {2041-1723},
  pmid      = {36737620},
  pmcid     = {PMC9898554}
}

@article{lippmann2025beyond,
  title        = {Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing},
  author       = {Lippmann, Peter and Gerhartz, Gerrit and Remme, Roman and Hamprecht, Fred A.},
  booktitle    = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year         = {2025},
  note         = {Poster, arXiv preprint arXiv:2405.15389v3},
  url          = {https://openreview.net/forum?id=vDp6StrKIq},
  archivePrefix = {arXiv},
  eprint       = {2405.15389}
}

@article{lawrence2025detecting,
  title        = {Detecting Symmetry-Breaking in Molecular Data Distributions},
  author       = {Lawrence, Hannah and Hofgard, Elyssa and Chen, Yuxuan and Smidt, Tess and Walters, Robin},
  booktitle    = {AI4Mat Workshop at the International Conference on Learning Representations (ICLR)},
  year         = {2025},
  url          = {https://openreview.net/forum?id=yEvdOXW5iY#discussion},
}



@article{gao2023bayesian,
  title={Bayesian Conditional Diffusion Models for Versatile Spatiotemporal Turbulence Generation},
  author={Gao, Han and Han, Xu and Fan, Xiantao and Sun, Luning and Liu, Li-Ping and Duan, Lian and Wang, Jian-Xun},
  journal={arXiv preprint arXiv:2311.07896},
  year={2023}
}


@article{van2023learning,
  title={Learning Layer-wise Equivariances Automatically using Gradients},
  author={van der Ouderaa, Tycho FA and Immer, Alexander and van der Wilk, Mark},
  journal={arXiv preprint arXiv:2310.06131},
  year={2023}
}

@article{fahle2001symmetry,
  title={Symmetry breaking},
  author={Fahle, Torsten and Schamberger, Stefan and Sellmann, Meinolf},
  booktitle={Principles and Practice of Constraint Programming—CP 2001: 7th International Conference, CP 2001 Paphos, Cyprus, November 26--December 1, 2001 Proceedings 7},
  pages={93--107},
  year={2001},
  organization={Springer}
}

@article{weinberg1976implications,
  title={Implications of dynamical symmetry breaking},
  author={Weinberg, Steven},
  journal={Physical Review D},
  volume={13},
  number={4},
  pages={974},
  year={1976},
  publisher={APS}
}

@article{beekman2019introduction,
  title={An introduction to spontaneous symmetry breaking},
  author={Beekman, Aron and Rademaker, Louk and van Wezel, Jasper},
  journal={SciPost Physics Lecture Notes},
  pages={011},
  year={2019}
}

@misc{tagawa2023symmetry,
  title={Symmetry in Fluid Flow},
  author={Tagawa, Toshio},
  journal={Symmetry},
  volume={15},
  number={3},
  pages={653},
  year={2023},
  publisher={MDPI}
}

@article{landau1936orderparam,
 title = {The Theory of Phase Transitions},
 author = {Landau, Lev},
 journal = {Nature},
 volume = {138},
 number = {3498},
 pages = {840-841},
 year = {1936}
 }

@incollection{Castellani2021symmbreak,
	author = {Castellani, Elena and Dardashti, Radin},
	booktitle = {The Routledge Companion to Philosophy of Physics},
	editor = {Eleanor Knox and Alistair Wilson},
	publisher = {Routledge},
	title = {Symmetry Breaking},
	year = {2021}
}

@Inbook{Dresselhaus2008,
title="Character of a Representation",
bookTitle="Group Theory",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="29--55",
doi="10.1007/978-3-540-32899-5_3",
url="https://doi.org/10.1007/978-3-540-32899-5_3"
}

@online{charactertables,
  author = {A. Gelessus},
  title = {Character tables for chemically important point groups},
  year = 2023,
  url = {http://symmetry.jacobs-university.de/},
  urldate = {2023-11-10}
}

@online{charactertableOh,
  author = {A. Gelessus},
  title = {Character table for point group Oh},
  year = 2023,
  url = {http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=904&option=4},
  urldate = {2023-11-13}
}
@article{dobriban2018,
  title={Distributed linear regression by averaging},
  author={Edgar Dobriban and Yueqi Sheng},
  journal={The Annals of Statistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:88517039}
}

@misc{tibshirani,
  author        = {Ryan Tibshirani},
  title         = {UC Berkeley Stat 241B, Lecture Notes: Overparametrized Regression: Ridgeless Interpolation},
  year          = {2024},
  url={https://www.stat.berkeley.edu/~ryantibs/statlearn-s24/lectures/ridgeless.pdf}

}

@misc{jzv,
  author        = {Jacob A.\ Zavatone-Veth},
  title         = {Harvard {APMTH} 226: Lecture notes on the inductive biases of high-dimensional ridge regression},
  year          = {2024},
  url={https://jzv.io/assets/pdf/am226_generalization_in_ridge_regression_lecture_notes.pdf}
}

@phdthesis{patil2022,
author = "Pratik Patil",
school = "Carnegie Mellon University",
title = "{Facets of regularization in high-dimensional learning: Cross-validation, risk monotonization, and model complexity}",
year = "2022",
month = "12",
url = "https://kilthub.cmu.edu/articles/thesis/Facets_of_regularization_in_high-dimensional_learning_Cross-validation_risk_monotonization_and_model_complexity/21692822",
doi = "10.1184/R1/21692822.v1"
}


@article{caponnetto2007,
	abstract = {We develop a theoretical analysis of the performance of the regularized least-square algorithm on a reproducing kernel Hilbert space in the supervised learning setting. The presented results hold in the general framework of vector-valued functions; therefore they can be applied to multitask problems. In particular, we observe that the concept of effective dimension plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples. Moreover, a complete minimax analysis of the problem is described, showing that the convergence rates obtained by regularized least-squares estimators are indeed optimal over a suitable class of priors defined by the considered kernel. Finally, we give an improved lower rate result describing worst asymptotic behavior on individual probability measures rather than over classes of priors.},
	author = {Caponnetto, A. and De Vito, E.},
	date = {2007/07/01},
	date-added = {2025-05-21 15:22:47 +0100},
	date-modified = {2025-05-21 15:22:47 +0100},
	doi = {10.1007/s10208-006-0196-8},
	id = {Caponnetto2007},
	isbn = {1615-3383},
	journal = {Foundations of Computational Mathematics},
	number = {3},
	pages = {331--368},
	title = {Optimal Rates for the Regularized Least-Squares Algorithm},
	url = {https://doi.org/10.1007/s10208-006-0196-8},
	volume = {7},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1007/s10208-006-0196-8}}


@article{sriperumbudur2009onintegral,
  author  = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Sch\"olkopf, Bernhard and Lanckriet, Gert R. G.},
  title   = {On Integral Probability Metrics, {$\phi$}-Divergences and Binary Classification},
  journal = {arXiv preprint arXiv:0901.2698},
  year    = {2009},
  url     = {https://arxiv.org/abs/0901.2698}
}

@article{cohen2019learning,
  author    = {Deborah Cohen and Amit Daniely and Amir Globerson and Gal Elidan},
  title     = {Learning Rules-First Classifiers},
  booktitle = {AISTATS},
  series    = {Proceedings of Machine Learning Research},
  year      = {2019},
  month     = {16--18 Apr},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v89/cohen19a/cohen19a.pdf}
}

@article{benton2020learning,
  title     = {Learning Invariances in Neural Networks},
  author    = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew G.},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  pages     = {21732--21744},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11882},
  note      = {arXiv preprint arXiv:2010.11882}
}

@article{nguyen2009onsurrogate,
  author  = {Nguyen, Xuan\,Long and Wainwright, Martin\,J. and Jordan, Michael\,I.},
  title   = {On surrogate loss functions and {$f$}-divergences},
  journal = {The Annals of Statistics},
  volume  = {37},
  number  = {2},
  pages   = {876--904},
  year    = {2009},
  doi     = {10.1214/08-AOS595},
  url     = {https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-2/On-surrogate-loss-functions-and-f-divergences/10.1214/08-AOS595.full}
}

@article{rubio2011,
title = {Spectral convergence for a general class of random matrices},
journal = {Statistics \& Probability Letters},
volume = {81},
number = {5},
pages = {592-602},
year = {2011},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2011.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167715211000113},
author = {Francisco Rubio and Xavier Mestre},
keywords = {Random matrix theory, Stieltjes transform, Multivariate statistics, Sample covariance matrix, Separable covariance model},
abstract = {Let X be an M×N complex random matrix with i.i.d. entries having mean zero and variance 1/N and consider the class of matrices of the type B=A+R1/2XTXHR1/2, where A, R and T are Hermitian nonnegative definite matrices, such that R and T have bounded spectral norm with T being diagonal, and R1/2 is the nonnegative definite square root of R. Under some assumptions on the moments of the entries of X, it is proved in this paper that, for any matrix Θ with bounded trace norm and for each complex z outside the positive real line, Tr[Θ(B−zIM)−1]−δM(z)→0 almost surely as M,N→∞ at the same rate, where δM(z) is deterministic and solely depends on Θ,A,R and T. The previous result can be particularized to the study of the limiting behavior of the Stieltjes transform as well as the eigenvectors of the random matrix model B. The study is motivated by applications in the field of statistical signal processing.}
}

@article{dobriban2020,
  title = 	 {One-shot Distributed Ridge Regression in High Dimensions},
  author =       {Sheng, Yue and Dobriban, Edgar},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8763--8772},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sheng20a/sheng20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sheng20a.html},
  abstract = 	 {To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental problem in this area: How to do ridge regression in a distributed computing environment? We study one-shot methods constructing weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high dimensional model where each predictor has a small effect, we discover several new phenomena including that the efficiency depends strongly on the signal strength, but does not degrade with many workers, the risk decouples over machines, and the unexpected consequence that the optimal weights do not sum to unity. We also propose a new optimally weighted one-shot ridge regression algorithm. Our results are supported by simulations and real data analysis.}
}


@book{bach2024learning,
  title={Learning Theory from First Principles},
  author={Bach, F.},
  isbn={9780262381369},
  lccn={2024017314},
  series={Adaptive Computation and Machine Learning series},
  url={https://books.google.co.uk/books?id=R_T8EAAAQBAJ},
  year={2024},
  publisher={MIT Press}
}

@online{charactertableD4h,
  author = {A. Gelessus},
  title = {Character table for point group D4h},
  year = 2023,
  url = {http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=604&option=4},
  urldate = {2023-11-13}
}

@article{benton2020,
 author = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17605--17616},
 publisher = {Curran Associates, Inc.},
 title = {Learning Invariances in Neural Networks from Training Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/cc8090c4d2791cdd9cd2cb3c24296190-Paper.pdf},
 volume = {33},
 year = {2020}
}
@article{romero2022,
 author = {Romero, David W. and Lohit, Suhas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {36466--36478},
 publisher = {Curran Associates, Inc.},
 title = {Learning Partial Equivariances From Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ec51d1fe4bbb754577da5e18eb54e6d1-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{
gidaris2018unsupervised,
title={Unsupervised Representation Learning by Predicting Image Rotations},
author={Spyros Gidaris and Praveer Singh and Nikos Komodakis},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1v4N2l0-},
}

@article{
charvin2023,
title={Towards Information Theory-Based Discovery of Equivariances},
author={Hippolyte Charvin and Nicola Catenacci Volpi and Daniel Polani},
booktitle={NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations},
year={2023},
url={https://openreview.net/forum?id=oD8DD5jQ5I}
}

@article{ouderaa2022,
  title = 	 {Learning invariant weights in neural networks},
  author =       {van der Ouderaa, Tycho F.A. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1992--2001},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/ouderaa22a/ouderaa22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/ouderaa22a.html},
  abstract = 	 {Assumptions about invariances or symmetries in data can significantly increase the predictive power of statistical models. Many commonly used machine learning models are constraint to respect certain symmetries, such as translation equivariance in convolutional neural networks, and incorporating other symmetry types is actively being studied. Yet, learning invariances from the data itself remains an open research problem. It has been shown that the marginal likelihood offers a principled way to learn invariances in Gaussian Processes. We propose a weight-space equivalent to this approach, by minimizing a lower bound on the marginal likelihood to learn invariances in neural networks, resulting in naturally higher performing models.}
}


@article{urbano2024,
  title = 	 {Self-supervised detection of perfect and partial input-dependent symmetries},
  author =       {Urbano, Alonso and Romero, David W.},
  booktitle = 	 {Proceedings of the Geometry-grounded Representation Learning and Generative Modeling Workshop (GRaM)},
  pages = 	 {113--131},
  year = 	 {2024},
  editor = 	 {Vadgama, Sharvaree and Bekkers, Erik and Pouplin, Alison and Kaba, Sekou-Oumar and Walters, Robin and Lawrence, Hannah and Emerson, Tegan and Kvinge, Henry and Tomczak, Jakub and Jegelka, Stephanie},
  volume = 	 {251},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v251/main/assets/urbano24a/urbano24a.pdf},
  url = 	 {https://proceedings.mlr.press/v251/urbano24a.html},
  abstract = 	 {Group equivariance can overly constrain models if the symmetries in the group differ from those observed in data. While common methods address this by determining the appropriate level of symmetry at the dataset level, they are limited to supervised settings and ignore scenarios in which multiple levels of symmetry co-exist in the same dataset. In this paper, we propose a method able to detect the level of symmetry of each input without the need for labels. Our framework is general enough to accommodate different families of both continuous and discrete symmetry distributions, such as arbitrary unimodal, symmetric distributions and discrete groups. We validate the effectiveness of our approach on synthetic datasets with different per-class levels of symmetries, and demonstrate practical applications such as the detection of out-of-distribution symmetries.}
}


@article{
ouderaa2022relaxing,
title={Relaxing Equivariance Constraints with Non-stationary Continuous Filters},
author={Tycho F.A. van der Ouderaa and David W. Romero and Mark van der Wilk},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=5oEk8fvJxny}
}

@article{miao2023,
  title = 	 {Learning Instance-Specific Augmentations by Capturing Local Invariances},
  author =       {Miao, Ning and Rainforth, Tom and Mathieu, Emile and Dubois, Yann and Teh, Yee Whye and Foster, Adam and Kim, Hyunjik},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {24720--24736},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/miao23a/miao23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/miao23a.html},
  abstract = 	 {We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.}
}



@article{wang2024symmbreak,
  title={Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution},
  author={Wang, Rui and Hofgard, Elyssa and Walters, Robin and Smidt, Tess},
  journal={arXiv preprint arXiv:2310.02299},
  year={2024}
}

@software{e3nnsoftware,
  author       = {Mario Geiger and
                  Tess Smidt and
                  Alby M. and
                  Benjamin Kurt Miller and
                  Wouter Boomsma and
                  Bradley Dice and
                  Kostiantyn Lapchevskyi and
                  Maurice Weiler and
                  Michał Tyszkiewicz and
                  Simon Batzner and
                  Dylan Madisetti and
                  Martin Uhrin and
                  Jes Frellsen and
                  Nuri Jung and
                  Sophia Sanborn and
                  Mingjian Wen and
                  Josh Rackers and
                  Marcel Rød and
                  Michael Bailey},
  title        = {Euclidean neural networks: e3nn},
  month        = apr,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {0.5.0},
  doi          = {10.5281/zenodo.6459381},
  url          = {https://doi.org/10.5281/zenodo.6459381}
}

@article{Lin2024,
  author  = {Chi-Heng Lin and Chiraag Kaushik and Eva L. Dyer and Vidya Muthukumar},
  title   = {The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {91},
  pages   = {1--85},
  url     = {http://jmlr.org/papers/v25/22-1312.html}
}

@article{fang2024phonon,
	title = {Phonon predictions with {E}(3)-equivariant graph neural networks},
	url = {https://arxiv.org/abs/2403.11347v1},
	author = {Fang, Shiang and Geiger, Mario and Checkelsky, Joseph G. and Smidt, Tess},
        journal = {arXiv preprint: arXiv:2403.11347},
        year = {2024}
}

@book{zee2016group,
	series = {In a nutshell},
	title = {Group theory in a nutshell for physicists},
	isbn = {978-0-691-16269-0},
	url = {https://books.google.com/books?id=FWkujgEACAAJ},
	publisher = {Princeton University Press},
	author = {Zee, A.},
	year = {2016},
	note = {tex.lccn: 2015037408},
}

@article{brehmer2024doesequivariancematterscale,
      title={Does equivariance matter at scale?}, 
      author={Johann Brehmer and Sönke Behrends and Pim de Haan and Taco Cohen},
      year={2024},
      eprint={2410.23179},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.23179}, 
}

@article{rMD17,
doi = {10.1088/2632-2153/abba6f},
url = {https://dx.doi.org/10.1088/2632-2153/abba6f},
year = {2020},
month = {oct},
publisher = {IOP Publishing},
volume = {1},
number = {4},
pages = {045018},
author = {Christensen, Anders S and von Lilienfeld, O Anatole},
title = {On the role of gradients for machine learning of molecular energies and forces},
journal = {Machine Learning: Science and Technology}
}

@misc{murphy2022implicitpdfnonparametricrepresentationprobability,
      title={Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold}, 
      author={Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},
      year={2022},
      eprint={2106.05965},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.05965}, 
}

@misc{atomicarchitects_datasets,
  author       = {Atomic Architects},
  title        = {Datasets},
  year         = {2023},
  howpublished = {\url{https://github.com/atomicarchitects/datasets}},
  note         = {Accessed: 2025-05-23}
}

@article{kernelenvs,
  title = {On representing chemical environments},
  author = {Bart\'ok, Albert P. and Kondor, Risi and Cs\'anyi, G\'abor},
  journal = {Phys. Rev. B},
  volume = {87},
  issue = {18},
  pages = {184115},
  numpages = {16},
  year = {2013},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.87.184115},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.87.184115}
}

@misc{klee2023imagespherelearningequivariant,
      title={Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction}, 
      author={David M. Klee and Ondrej Biza and Robert Platt and Robin Walters},
      year={2023},
      eprint={2302.13926},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.13926}, 
}

@article{md17,
	title = {Machine learning of accurate energy-conserving molecular force fields},
	rights = {Copyright © 2017, The Authors},
	url = {https://www.science.org/doi/10.1126/sciadv.1603015},
	doi = {10.1126/sciadv.1603015},
	abstract = {The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields.},
	journaltitle = {Science Advances},
	author = {Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E. and Poltavsky, Igor and Schütt, Kristof T. and Müller, Klaus-Robert},
	urldate = {2025-01-31},
	date = {2017-05},
    year={2017},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{
twosampletest,
title={Revisiting Classifier Two-Sample Tests},
author={David Lopez-Paz and Maxime Oquab},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJkXfE5xx}
}

@article{qm9,
	title = {{MoleculeNet}: a benchmark for molecular machine learning},
	volume = {9},
	shorttitle = {{MoleculeNet}},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC5868307/},
	doi = {10.1039/c7sc02664a},
	abstract = {A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.},
	language = {en},
	number = {2},
	urldate = {2025-02-02},
	journal = {Chemical Science},
	author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
	month = oct,
	year = {2017},
	pmid = {29629118},
	pages = {513},
}

@article{ocp_dataset,
    author = {Chanussot*, Lowik and Das*, Abhishek and Goyal*, Siddharth and Lavril*, Thibaut and Shuaibi*, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
    title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
    journal = {ACS Catalysis},
    year = {2021},
    doi = {10.1021/acscatal.0c04525},
}

@article{wang2022surprising,
  title={The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry},
  author={Wang, Dian and Park, Jung Yeon and Sortur, Neel and Wong, Lawson LS and Walters, Robin and Platt, Robert},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{
cohen2018spherical,
title={Spherical {CNN}s},
author={Taco S. Cohen and Mario Geiger and Jonas Köhler and Max Welling},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hkbd5xZRb},
}

article{wang2022surprising,
  title={The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry},
  author={Dian Wang and Jung Yeon Park and Neel Sortur and Lawson L. S. Wong and Robin Walters and Robert W. Platt},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.09231},
  url={https://api.semanticscholar.org/CorpusID:253581623}
}

@article{
gruver2024finetuned,
title={Fine-Tuned Language Models Generate Stable Inorganic Materials as Text},
author={Nate Gruver and Anuroop Sriram and Andrea Madotto and Andrew Gordon Wilson and C. Lawrence Zitnick and Zachary Ward Ulissi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vN9fpfqoP1}
}

@article{wang2024general,
author = {Wang, Dian and Zhu, Xupeng and Park, Jung Yeon and Jia, Mingxi and Su, Guanang and Platt, Robert and Walters, Robin},
title = {A general theory of correct, incorrect, and extrinsic equivariance},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1740},
numpages = {24},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{shao2024theory,
author = {Shao, Han and Montasser, Omar and Blum, Avrim},
title = {A theory of PAC learnability under transformation invariances},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1017},
numpages = {13},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{hofgard2024relaxe3nn,
  title={Relaxed Equivariant Graph Neural Networks},
  author={Hofgard, Elyssa and Wang, Rui and Walters, Robin and Smidt, Tess E.},
  journal={arXiv preprint arXiv:2407.20471},
  year={2024}
}

@article{romero2024,
    author = {Urbano, Alonso and Romero, David W.},
    title = {Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries},
    journal ={arXiv preprint arXiv:2312.12223},
    year = {2024} 
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{igashov2024equivariant,
  title        = {Equivariant 3D-conditional diffusion model for molecular linker design},
  author       = {Igashov, Ilia and St{\"a}rk, Hannes and Vignac, Cl{\'e}ment and Schneuing, Arne and Satorras, Victor Garcia and Frossard, Pascal and Welling, Max and Bronstein, Michael and Correia, Bruno},
  year         = 2024,
  journal      = {Nature Machine Intelligence},
  publisher    = {Nature Publishing Group UK London},
  pages        = {1--11}
}

@article{wang2024equivariant,
  title={Equivariant Diffusion Policy},
  author={Wang, Dian and Hart, Stephen and Surovik, David and Kelestemur, Tarik and Huang, Haojie and Zhao, Haibo and Yeatman, Mark and Wang, Jiuguang and Walters, Robin and Platt, Robert},
  journal={arXiv preprint arXiv:2407.01812},
  year={2024}
}

@article{wangincorporating,
  title={Incorporating Symmetry into Deep Dynamics Models for Improved Generalization},
  author={Wang, Rui and Walters, Robin and Yu, Rose},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{esteves2019equivariant,
  title={Equivariant multi-view networks},
  author={Esteves, Carlos and Xu, Yinshuang and Allen-Blanchette, Christine and Daniilidis, Kostas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1568--1577},
  year={2019}
}

@article{soleymaniRobustKernelStatistical2025,
  title = {A {{Robust Kernel Statistical Test}} of {{Invariance}}: {{Detecting Subtle Asymmetries}}},
  shorttitle = {A {{Robust Kernel Statistical Test}} of {{Invariance}}},
  booktitle = {The {{Second Conference}} on {{Parsimony}} and {{Learning}} ({{Recent Spotlight Track}})},
  author = {Soleymani, Ashkan and Tahmasebi, Behrooz and Jegelka, Stefanie and Jaillet, Patrick},
  year = {2025},
  month = mar,
  urldate = {2025-07-19},
  abstract = {While invariances naturally arise in almost any type of real-world data, no efficient and robust test exists for detecting them in observational data for arbitrarily given group actions. We tackle this problem by studying measures of invariance that can capture even negligible underlying patterns. Our first contribution is to show that, while detecting subtle asymmetries is computationally intractable, a randomized method can be used to estimate robust closeness measures to invariance within constant factors. This provides a general framework for robust statistical tests of invariance. In addition, we focus on kernel methods and propose deterministic algorithms for robust testing with respect to both finite and infinite groups, accom- panied by a rigorous analysis of their convergence rates and sample complexity. Finally, we revisit the general framework in the specific case of kernel methods, showing that recent closeness measures to invariance, defined via group averaging, are provably robust, leading to powerful randomized algorithms.}
}
